What is ETL(Extract Transform Load)?

As the amount of data, data sources, and data types at organizations grow, the importance of making use of that data in analytics, 
data science and machine learning initiatives to derive business insights grows as well. The need to prioritize these initiatives 
puts increasing pressure on the data engineering teams because processing the raw, messy data into clean, fresh, reliable data is a 
critical step before these initiatives can be pursued. ETL, which stands for extract, transform, and load, is the process data engineers 
use to extract data from different sources, transform the data into a  usable and trusted resource, and load that data into the systems 
end-users can access and use downstream to solve business problems.


How Does ETL Work?
Extract
The first step of this process is extracting data from the target sources that are usually heterogeneous such as business systems, APIs, sensor data, marketing tools, and transaction databases, and others. As you can see, some of these data types are likely to be the structured outputs of widely used systems, while others are semi-structured JSON server logs.There are different ways to perform the extraction: Three Data Extraction methods:

Partial Extraction – The easiest way to obtain the data is if the if the source system notifies you when a record has been changed
Partial Extraction (with update notification) - Not all systems can provide a notification in case an update has taken place; however, they can point to those records that have been changed and provide an extract of such records.
Full extract – There are certain systems that cannot identify which data has been changed at all. In this case, a full extract is the only possibility to extract the data out of the system. This method requires having a copy of the last extract in the same format so you can identify the changes that have been made.
Transform
The second step consists of transforming the raw data that has been extracted from the sources into a format that can be used by different applications. In this stage, data gets cleansed, mapped and transformed, often to a specific schema, so it meets operational needs. This process entails several types of transformation that ensure the quality and integrity of data Data is not usually loaded directly into the target data source, but instead it is common to have it uploaded into a staging database. This step ensures a quick roll back in case something does not go as planned. During this stage, you have the possibility to generate audit reports for regulatory compliance, or diagnose and repair any data issues.

Load
Finally, the load function is the process of writing converted data from a staging area to a target database, which may or may not have previously existed. Depending on the requirements of the application, this process may be either quite simple or intricate. Each of these steps can be done with ETL tools or custom code.
